---
title: "BST260 Final Project RMarkdown"
author: "Yanran Li, Sean Gao, Yunan Zhao"
date: "2020/12/8"
output: html_document
---

## Customer Revenue Data Exploration
```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(ggplot2)
library(zoo)
library(h2o)
library(caret)
library(xgboost)
library(jsonlite)
library(knitr)
library(Rmisc)
library(scales)
library(countrycode)
library(magrittr)
library(mice) # for missing value
library(broom) # for tidy statistical objects
library(splines2) # for model smoothing
library(gam) # for model smoothing
library(plyr) # for split-apply-combien-pattern in R
library(gapminder) # to convert the 
library(xgboost)
library(caret)
library(rsample)
library(randomForest)

```

```{r, message=FALSE}
# Read the Customer Revenue data for further reference:
# training data: train
# testing data: test
# submission data: subm
train <- read_csv("train.csv")
test <- read_csv("test.csv")
submission <- read_csv("sample_submission.csv")
```

## Reshaping Data
```{r}
set.seed(0)
head(test)
unique(train$socialEngagementType)
```

```{r}
# printing out the first five rows of te and tr for further exploration
head(train,5)
head(test,5)
```

As we can see from above, device, geoNetwork, trafficSource, totals are all in JSON format, hence the next step we need to is to deal with the JSON data.

```{r message=FALSE, warning=FALSE, results='hide'}

# here since I am referring back to the link:
# https://www.kaggle.com/mrlong/r-flatten-json-columns-to-make-single-data-frame
# https://www.kaggle.com/kailex/r-eda-for-gstore-glm-keras-xgb
# in terms of transfroming the netted json file into single data frame
# as we can see from the above dataframe, for the json data, it is delimited by ,

train_device <- paste("[", paste(train$device, collapse = ","), "]") %>% fromJSON(flatten = T)
train_geoNetwork <- paste("[", paste(train$geoNetwork, collapse = ","), "]") %>% fromJSON(flatten = T)
train_totals <- paste("[", paste(train$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
train_trafficSource <- paste("[", paste(train$trafficSource, collapse = ","), "]") %>% fromJSON(flatten = T)

test_device <- paste("[", paste(test$device, collapse = ","), "]") %>% fromJSON(flatten = T)
test_geoNetwork <- paste("[", paste(test$geoNetwork, collapse = ","), "]") %>% fromJSON(flatten = T)
test_totals <- paste("[", paste(test$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
test_trafficSource <- paste("[", paste(test$trafficSource, collapse = ","), "]") %>% fromJSON(flatten = T)
```

```{r message=FALSE, warning=FALSE, results='hide'}
# sanity check upon the dealing with the json file
head(train_device, 5)
head(test_device, 5)
```

```{r message=FALSE, warning=FALSE, results='hide'}

# after we got the json-dealt file, we need to combine all of the columns for the four original columns
# here we will apply cbind, i.e column bind
# some codes idea come from this link:
# https://medium.com/coinmonks/merging-multiple-dataframes-in-r-72629c4632a3

train <- train  %>%  cbind(train_device, train_geoNetwork, train_totals, train_trafficSource )
# also we need to get rid of the original four columns
train <- train  %>%  select(-device,-geoNetwork, -trafficSource, -totals)


test <- test  %>%  cbind(test_device, test_geoNetwork, test_totals, test_trafficSource )
# also we need to get rid of the original four columns
test <- test  %>%  select(-device,-geoNetwork, -trafficSource, -totals) 

```

```{r message=FALSE, warning=FALSE, results='hide'}
# sanity check to see whether we get rid of all of the previous json problems
# also here we print out the number of columns for train and test
head(train, 5)
head(test, 5)
ncol(train)
ncol(test)
```

As we can see from the above checking, we do see that there are some null values, and the testing dataset has two fewer columns than the train ones. Therefore, we would like to further discover which are those different columns

```{r message=FALSE, warning=FALSE, results='hide'}
# sanity check to see whether we get rid of all of the previous json problems
# also here we print out the number of columns for train and test
train_col_names <- names(train)
test_col_names <- names(test)
# https://stackoverflow.com/questions/31573087/difference-between-two-vectors-in-r
setdiff(train_col_names, test_col_names)
```

As we can see from above, transactionRevenue is the target variable, which is for sure missing in the testing for campaignCode, for now I think it is okay for us to remove the corresponding column

```{r}
train <- train %>% select(-campaignCode)
```

```{r}
# double check to make sure the corresponding column has been deleted successfully
ncol(train)
```

```{r, warning=FALSE}
# reference: https://dplyr.tidyverse.org/reference/n_distinct.html
# here we want to return the number of unique values for each column in the dataset
col_unique_val <- sapply(train, n_distinct)
col_unique_val

# now, select all fo the values with only one distinct value, i.e those one with constant values
# for these columns, we will get rid of those
constant_col <- names(col_unique_val[col_unique_val==1])
# now we need to delete corresponding columns from both train and test
train <- train %>% select(-constant_col)
test <- test %>% select(-constant_col)
```

Before examining data analysis, we would like to deal with the missing values in our dataset. 
```{r}
sum(is.na(train))
sum(is.na(test))
```

Though the number of missing values in our dataset looks scary, it is probably owing to our large dataset. Besides, there are some colums that we do not really interested in, so we would like to locate how these missing values distribute in our dataset.
```{r nas0, result='asis', echo=TRUE}
miss_train <- md.pattern(train, plot = FALSE, rotate.name = TRUE)
m_train <- miss_train[order(miss_train[,36], decreasing = TRUE),]
colnames(m_train)[36] <- "sum"
m_train[1,]
## Here, we print the number of missing values n each column, and it would be a reference about which variable that we would like to choose afterwards
```

## Data Wrangling

We need to convert some features to their natural representation, including date, hits(integer, provides a record of all page visits), pageviews, bounces, newVisits, transactionRevenue.

```{r tf1, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
train <- train %>%
  mutate(date = ymd(date),
         hits = as.integer(hits),
         pageviews = as.integer(pageviews),
         bounces = as.integer(bounces),
         newVisits = as.integer(newVisits),
         transactionRevenue = as.numeric(transactionRevenue))
         
test <- test %>%
  mutate(date = ymd(date),
         hits = as.integer(hits),
         pageviews = as.integer(pageviews),
         bounces = as.integer(bounces),
         newVisits = as.integer(newVisits))

``` 



This dataset keeps track of the transaction revenue reported for cities and regions within a given country. We would like to only look at the overall and other sub-regions within a given country, so we create a new data frame with three columns: country name, date, hits, and the transaction revenue in the country on that day. For now, we first dig more into the data for the following countries: the United States, China, South Korea, and Germany.

```{r}
new_train <- train %>%
  filter(country %in% c("United States", "South Korea", "Germany", "China")) %>%
  select(country, date, hits, transactionRevenue)
head(new_train)
```

```{r, message=FALSE}
tidy_train <- new_train %>%
  group_by(date, country) %>%
  summarize(hitss = sum(hits))
head(tidy_train)
```


By comparing the hits of customers between four countries, we notce that the United States has much higher page visits than all the other countries. This is probably because of the following reasons:
1) Customer in the US has differnt shopping style from people in other countries.
2) The Google Merchndise website favored the US customers. For example, some countries might have access issue to the website to protect local business.
3) The data collection in the US is more comprehensive.

#### Target variable
Firstly, we see **transactionRevenue** which is a sub-column of the **totals** JSON column. 
```{r target, result='asis', echo=TRUE}
y <- train$transactionRevenue
# train$transactionRevenue <- NULL
summary(y)
```

There exists 892138 NA values in **transactionRevenue**, which means only 1.27% rows have this value.


Secondly, we see **channelGrouping**, which is the channel via which the user came to the Store. We drew a histogram about the counts of channels among all the records in the training set.

```{r}
unique(train$channelGrouping)

train %>% ggplot(aes(channelGrouping)) + geom_bar() + ggtitle("The channel via which the user came to the Store") +
xlab("Channel") +
ylab("Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position="none") 
```

From the histogram above, the "Organic Search" had the most records, and then "Social", "Direct" followed.


## Regression
After having some general ideas about our dataset, we would like to make statistical model about customers' revune. Since our dataset is too large, we would like to have a truncated dataset to by filtering the original dataset to avoid long runtimes and computer crashing.

Here, we are interested in the condition in the United States.

```{r}
revenue_us <- train %>%
  select(c(channelGrouping, date,
           fullVisitorId, sessionId, visitId,
           visitNumber, visitStartTime,
           isMobile, deviceCategory, hits, browser, source, 
           continent, subContinent, country, region, metro, city,
           transactionRevenue)) %>%
  filter(country == "United States")

write.csv(x = revenue_us, file = "revenue_us.csv")
```

The ourcome Y in our study is the continuous customer revenue, and we would like to do prediction about it based on cutomers' characteristics.

```{r}
str(revenue_us)
```

The column "hits" is a continuous variable provides record of all page vistis for the cusomer, and "isMobile" is a categorical variable indicates whether the customer used mobile device to access the Store. First, we want to visualze the relationship between these two explanatory variables and the outcome revenue.

```{r}
scatter.smooth(revenue_us$hits, revenue_us$transactionRevenue, col="pink",
               main = "Relation between hits and revenue",
               xlab = "Number of the page vist", ylab = "Transaction revenue")
plot(factor(revenue_us$isMobile), revenue_us$transactionRevenue, col="purple", lwd = 2)
pairs(~ hits + transactionRevenue + factor(isMobile), data = revenue_us, main = " ")
```

```{r}
# Here, we fit a simple linear model Revenue ~ Hits in accordance with the scatter plot above
lm_hits <- lm(revenue_us$transactionRevenue ~ revenue_us$hits)
summary(lm_hits)$coef
```
The coefficient of hits is statistically significant based on this regression model, which indicates that hits is associated with the output revenue.

```{r}
# Redefine isMobile variable for ggplot input and run this multiple linear model
revenue_us$isMobile[revenue_us$isMobile == "TRUE"] = 1
revenue_us$isMobile[revenue_us$isMobile == "FALSE"] = 0
lm2 <- lm(transactionRevenue ~ hits + isMobile, data = revenue_us)
summary(lm2)$coef
```

```{r, warning=FALSE}
# Here, we have the plot of the revenue ~ hits + device
p <- ggplot(data = revenue_us,
            aes(x = hits, y = transactionRevenue, color = factor(isMobile))) +
  geom_point() +
  ggtitle("Transaction Revenue by hits + device") +
  scale_color_manual(values = c("blue", 'hot pink')) +
  theme(legend.title = element_blank()) +
  scale_fill_manual(values = c("0", "1"), name="Access Device", breaks = c("0", "1"),
                    labels=c("Not Mobile", "Mobile"))
p
```
```{r}
# To further explore the relationship between revenue and hits, we start to consider degree polynomials
hits.2 <- I(revenue_us$hits^2)
pairs(~ hits + transactionRevenue + hits.2, data = revenue_us, main = "Pairs")
```
```{r}
# Fit multiple linear regression model include quadratic hits term
lm3_sq = lm(transactionRevenue ~ hits + I(hits^2) + isMobile, data = revenue_us)
tidy(lm3_sq)

# Fit multiple linear regression model with interaction term
lm4 = lm(transactionRevenue ~ hits + I(hits^2) + isMobile + hits*isMobile, data = revenue_us)
tidy(lm4)
```

Thought the p-values from previous models are all significant, we should notice that these linear regression model has several assumptions. Now, we would like to check whethere the main assumptions are violated.

```{r}
# Residuals versus fitted plot
plot(fitted(lm4), residuals(lm4))
abline(a=0, b=0, col="red", lwd=2)

# QQ Norm plot
qqnorm(residuals(lm4))
qqline(residuals(lm4), col="red", lwd=2)
```

From the scatterplot of Residuals versus fitted values, there are few discernable outliers. The Q-Q plot also departures from straight line, which is statistical evidence of non-normality. Since the assumptions do not hold, we need to check our data and develop more appropriate models for the given data.

```{r}
# To reduce the skewness in the residuals, we transform the output Y
lm5_log <- lm(log(transactionRevenue) ~ hits + I(hits^2) + isMobile, data = revenue_us)
tidy(lm5_log)
plot(fitted(lm5_log), residuals(lm5_log))
abline(a=0, b=0, col="3", lwd=2)
qqnorm(residuals(lm5_log))
qqline(residuals(lm5_log), col="3", lwd=2)
```

By apply log transformation for the transaction revenue, we achieve the linearity of effect, and normalize the residuals. Now, the regression model satisfies the assumptions better. 

```{r, warning=FALSE}
# After developing the relationship between revenue, hits, and device, we would like to consider including other forms of covariates into our regression model. Before doing that, we delve into the dataset again to see other potential estimators
scatter.smooth(revenue_us$visitStartTime, log(revenue_us$transactionRevenue),
               col = "brown",
               xlab = "Number of the page vist",
               ylab = "Transaction revenue",
               main = "Relation between visit starttime and revenue")

p2 <- ggplot(data = revenue_us,
            aes(x = channelGrouping, y = log(transactionRevenue))) +
              geom_boxplot() +
  xlab("Access channel") +
  ylab("Transaction revenue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
p2
```
According to the scatterplot and the boxplot, it seems that both visitStartTime and channelGroupng do not have significant influence on the value of transaction revenue, so we will keep using "hits" and "isMobile" as the predictors.

```{r}
# fit degree = df = 3, 6, 16
lm5.poly3 <- lm(log(transactionRevenue) ~ poly(hits, 3) + isMobile, data = revenue_us)
lm5.bs3 <- lm(log(transactionRevenue) ~ bs(hits, df = 3) + isMobile, data = revenue_us)
lm5.bs6 <- lm(log(transactionRevenue) ~ bs(hits, df = 6) + isMobile, data = revenue_us)
lm5.bs16 <- lm(log(transactionRevenue) ~ bs(hits, df = 16) + isMobile, data = revenue_us)

tidy(lm5.poly3)
tidy(lm5.bs3)
tidy(lm5.bs6)
tidy(lm5.bs16)
```

Beforehand, we did log transformation for the output Y, and here we would like to transform our predictors. By including predictive variable "hits" in the form of polynomial function with degree = 3, the adjusted R-squared increase indicating a better prediction to some extent. In addition, we apply Spline model to smooth the overall function, and the adjusted R-squared increases as we have higher degrees of freedom in this study.







### Now, we will start working on the machine learning part

For this project specifically, our team is considering three regression based methods
The first one being bagging methods --> i.e. random forest regressor
The second one being boosting methods --> i.e. XGboost regressor

However, based on our previous finding, there are only 1.27% data without null values in terms of transaction revenue


#### Starting of the random forest regressor

Firstly, we need to get the corresponding data without the dependent variable which would be the 
Reference: http://corina.cc/article/28/
```{r}
# select corresponding independent variables and dependent variable
# reference: https://www.jamleecute.com/random-forests-%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97/
# first conduct train test split
set.seed(123)

#head(train,5)
revenue<-subset(train, (!is.na(train[,"transactionRevenue"])))

split <- initial_split(data=revenue, prop = 0.7)
revenue_train <- analysis(split)
revenue_test <- assessment(split)

nrow(revenue_train)
nrow(revenue_test)
# head(revenue_train,5)

revenue_train_rf <- revenue_train %>%
  select(c(channelGrouping, date,
           visitId,
           visitNumber, visitStartTime,
           isMobile, deviceCategory, hits, browser, source, 
           continent, subContinent, metro, transactionRevenue))
# head(revenue_train_rf)

# note that in R, random forest is unable to handle a variable with 50+ categorical values
temp <- sapply(revenue_train_rf, n_distinct)
temp

# head(revenue_train_rf,5)


revenue_train_rf=revenue_train_rf %>% mutate_if(is.character, as.factor)

set.seed(123)

rf <- randomForest(transactionRevenue ~., data = revenue_train_rf, importance = TRUE, ntree=1000)
# somehow, the transactionRevenue is NA in our case here, which is pretty weird

# we can further plot the Mean Squared Error on the training dataset
plot(rf)

# we can also get the corresponding number of trees which lead to the smallest mse
which.min(rf$mse)


```



From the above output, we know that the best number of tree for the random forest in our case here is 9. In the meantime, random forest tends to outperform the linear regression model a lot, for two reasons we believe:
1) since random forest is a bagging based methods, hence through bagging, the overall accuracy/metric performance will be better than the others
2) since the data amount is relatively small, hence the machine learning might face overfitting problem.


### Starting of XGBoost
```{r}

# reference: https://www.datatechnotes.com/2020/08/regression-example-with-xgboost-in-r.html
# first conduct train test split
set.seed(123)

library(xgboost)
library(caret)
set.seed(12)


train_x = data.matrix(revenue_train_rf[,-14])
train_y = revenue_train_rf[,14]


revenue_test_rf <- revenue_test %>%
  select(c(channelGrouping, date,
           visitId,
           visitNumber, visitStartTime,
           isMobile, deviceCategory, hits, browser, source, 
           continent, subContinent, metro, transactionRevenue))

revenue_test_rf=revenue_test_rf %>% mutate_if(is.character, as.factor)


test_x = data.matrix(revenue_test_rf[, -14])
test_y = revenue_test_rf[, 14]

xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)

# now we are able to apply the XGBoost onto the data itself
xgbc = xgboost(data = xgb_train, nrounds=100)

```



From above data, we can see that XGBoost actually is able to outperform random forest model performance. In our opinion, there might be two reasons for this to happen:
1) random forest tends to perform better when there is categorical variables,  however, in our case, we get rid of all of the categorical values, hence XGBoost might be able to outperform.
2) with XGBoost being a boosting based methods, for this kinds of regression problem, it may be able to perform pretty well given that the model performance is contingent upon samll improvement step by step. 


























