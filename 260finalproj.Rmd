---
title: "BST260 Final Project RMarkdown"
author: "Yanran, Yunan"
date: "2020/12/3"
output: html_document
---

### Customer Revenue Data Exploration

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(ggplot2)
library(zoo)
library(mice)
library(dplyr)

library(h2o)
library(caret)
library(lme4)
library(ggalluvial)
library(xgboost)
library(jsonlite)
library(knitr)
library(Rmisc)
library(scales)
library(countrycode)
library(highcharter)
library(glmnet)
library(keras)
library(forecast)
library(magrittr)
```

### Load data
```{r, message=FALSE}
# Read the Customer Revenue data for further 

# for further reference:
# training data: train
# testing data: test
# submission data: subm

train <- read_csv("train.csv")
test <- read_csv("test.csv")
submission <- read_csv("sample_submission.csv")
```

### Reshaping Data

```{r}
# printing out the first five rows of te and tr for further exploration
head(train,5)
head(test,5)

```

As we can see from above, device, geoNetwork, trafficSource, totals are all in JSON format, hence the next step we need to is to deal with the JSON data.

```{r fun0, message=FALSE, warning=FALSE, results='hide'}

# here since I am referring back to the link:
# https://www.kaggle.com/mrlong/r-flatten-json-columns-to-make-single-data-frame
# https://www.kaggle.com/kailex/r-eda-for-gstore-glm-keras-xgb
# in terms of transfroming the netted json file into single data frame
# as we can see from the above dataframe, for the json data, it is delimited by ,

train_device <- paste("[", paste(train$device, collapse = ","), "]") %>% fromJSON(flatten = T)
train_geoNetwork <- paste("[", paste(train$geoNetwork, collapse = ","), "]") %>% fromJSON(flatten = T)
train_totals <- paste("[", paste(train$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
train_trafficSource <- paste("[", paste(train$trafficSource, collapse = ","), "]") %>% fromJSON(flatten = T)

test_device <- paste("[", paste(test$device, collapse = ","), "]") %>% fromJSON(flatten = T)
test_geoNetwork <- paste("[", paste(test$geoNetwork, collapse = ","), "]") %>% fromJSON(flatten = T)
test_totals <- paste("[", paste(test$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
test_trafficSource <- paste("[", paste(test$trafficSource, collapse = ","), "]") %>% fromJSON(flatten = T)
```

```{r fun0, message=FALSE, warning=FALSE, results='hide'}
# sanity check upon the dealing with the json file
head(train_device, 5)
head(test_device, 5)
```

```{r fun0, message=FALSE, warning=FALSE, results='hide'}

# after we got the json-dealt file, we need to combine all of the columns for the four original columns
# here we will apply cbind, i.e column bind
# some codes idea come from this link:
# https://medium.com/coinmonks/merging-multiple-dataframes-in-r-72629c4632a3

train <- train  %>%  cbind(train_device, train_geoNetwork, train_totals, train_trafficSource )
# also we need to get rid of the original four columns
train <- train  %>%  select(-device,-geoNetwork, -trafficSource, -totals)


test <- test  %>%  cbind(test_device, test_geoNetwork, test_totals, test_trafficSource )
# also we need to get rid of the original four columns
test <- test  %>%  select(-device,-geoNetwork, -trafficSource, -totals) 

```


```{r fun0, message=FALSE, warning=FALSE, results='hide'}
# sanity check to see whether we get rid of all of the previous json problems
# also here we print out the number of columns for train and test
head(train, 5)
head(test, 5)
ncol(train)
ncol(test)
```

As we can see from the above checking, we do see that there are some null values, and the testing dataset has two fewer columns than the train ones
Therefore, we would like to further discover which are those different columns

```{r fun0, message=FALSE, warning=FALSE, results='hide'}
# sanity check to see whether we get rid of all of the previous json problems
# also here we print out the number of columns for train and test
train_col_names <- names(train)
test_col_names <- names(test)
# https://stackoverflow.com/questions/31573087/difference-between-two-vectors-in-r
setdiff(train_col_names, test_col_names)
```

As we can see from above, transactionRevenue is the target variable, which is for sure missing in the testing 
for campaignCode, for now I think it is okay for us to remove the corresponding column

```{r}
train <- train %>% select(-campaignCode)
```

```{r}
# double check to make sure the corresponding column has been deleted successfully
ncol(train)
```

## Now, let's deal with columns with constant values and missing values
```{r}
# reference: https://dplyr.tidyverse.org/reference/n_distinct.html
# here we want to return the number of unique values for each column in the dataset
col_unique_val <- sapply(train, n_distinct)
all_of(constant_col)

# now, select all fo the values with only one distinct value, i.e those one with constant values
# for these columns, we will get rid of those
constant_col <- names(col_unique_val[col_unique_val==1])
# now we need to delete corresponding columns from both train and test
train <- train %>% select(-constant_col)
test <- test %>% select(-constant_col)
```

## Missing values

Before examine data analysis, we would like to deal with the missing values in our dataset. 
```{r}
sum(is.na(train))
sum(is.na(test))
```

Though the number of missing values in our dataset looks scary, it is probably owing to our large dataset. Besides, there are some colums that we do not really interested in, so we would like to locate how these missing values distribute in our dataset.

```{r nas0, result='asis', echo=TRUE}
miss_train <- md.pattern(train, plot = FALSE, rotate.name = TRUE)
m_train <- missing[order(missing[,36], decreasing = TRUE),]
colnames(m_train)[36] <- "sum"
m_train[1,]

```

Then, we do the same thing for the test dataset
```{r}
miss_test <- md.pattern(test, plot = FALSE, rotate.name = TRUE)
m_test <- missing[order(missing[,36],decreasing=TRUE),]
colnames(m_test)[36] <- "sum"
m_test[1,]

## Here, we print the number of missing values n each column, and it would be a reference about which variable that we would like to choose afterwards
```

## Data Wragling
We need to convert some features to their natural representation, including date, hits(integer, provides a record of all page visits), pageviews, bounces, newVisits, transactionRevenue.

```{r tf1, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
train <- train %>%
  mutate(date = ymd(date),
         hits = as.integer(hits),
         pageviews = as.integer(pageviews),
         bounces = as.integer(bounces),
         newVisits = as.integer(newVisits),
         transactionRevenue = as.numeric(transactionRevenue))
         
test <- test %>%
  mutate(date = ymd(date),
         hits = as.integer(hits),
         pageviews = as.integer(pageviews),
         bounces = as.integer(bounces),
         newVisits = as.integer(newVisits))

``` 

## Data Visualization

This dataset keeps track of the transaction revenue reported for cities and regions within a given country. We would like to only look at the overall and other sub-regions within a given country, so we create a new data frame with three columns: country name, date, and the transaction revenue in the country on that day. 

```{r}
library(plyr)
new_train <- train %>% 
  select(c(country, date, transactionRevenue)) %>%
  group_by(country, date) %>%
  filter(!is.na(transactionRevenue )) 
  ###  %>% summarize(cases = sum(transactionRevenue))
## total revenue 这里有点问题还

countries <- "United States"
new_train %>% filter(country == "United States") %>%
   ggplot(aes(x = date, y = transactionRevenue, color = country)) + 
   geom_line() +
   xlab("Date") + 
   ylab("Rransaction Revenue") +
   ggtitle("Global Data of the Transaction Revenue")


```


Using your new data frame, make a line plot of the Transactiion revenues vs. date for the following countries: the United States.




## Target variable
As a target variable we use **transactionRevenue** which is a sub-column of the **totals** JSON column. It looks like
this variable is multiplied by $10^6$.

```{r target, result='asis', echo=TRUE}
y <- train$transactionRevenue
# train$transactionRevenue <- NULL
summary(y)
```

We can safely replace **NA** values with 0.
```{r, result='asis', echo=TRUE}
y[is.na(y)] <- 0
summary(y)
```


```{r, result='asis', echo=FALSE}
p1 <- as_tibble(y) %>% 
  ggplot(aes(x = log1p(value))) +
  geom_histogram(bins = 30, fill="steelblue") + 
  labs(x = "transaction revenue") +
  theme_minimal()

p2 <- as_tibble(y[y>0]) %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill="steelblue") + 
  labs(x = "non-zero transaction revenue") +
  theme_minimal()

multiplot(p1, p2, cols = 2)

as_tibble(log1p(y[y>0] / 1e6)) %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill="steelblue") + 
  labs(x = "log(non-zero transaction revenue / 1e6)") +
  theme_minimal()
```

The target variable has a wide range of values. Its distribution is right-skewed. For modelling we will use
log-transformed target. 

BTW, only `r round(length(y[y!=0]) / length(y) * 100, 2)`% 
of all transactions have non-zero revenue:

```{r rev0, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  ggplot(aes(x = 1:length(value), y = value)) +
  geom_point(color = "steelblue",alpha=0.4, size=0.8) +
  theme_minimal() +
  scale_y_continuous(name="revenue", labels = comma) + 
  scale_x_continuous(name="index", labels = comma) +
  theme(legend.position="none")
```

The next figure shows that users who came via **Affiliates** and **Social**
channels do not generate revenue. The most profitable channel is **Referral**: 

```{r rev1, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
train %>% 
  bind_cols(as_tibble(y)) %>% 
  group_by(channelGrouping) %>% 
  summarise(revenue = sum(value)) %>%
  ggplot(aes(x = channelGrouping, y = revenue)) +
  geom_point(color="steelblue", size=2) +
  theme_minimal() +
  scale_y_continuous(labels = comma) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

Also usually first visit users generate more total revenue:

```{r rev2, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  group_by(visitNumber) %>% 
  summarise(revenue = sum(value)) %>%
  ggplot(aes(x = visitNumber, y = revenue)) +
  geom_point(color="steelblue", size=0.5) +
  theme_minimal() +
  scale_x_continuous(breaks=c(1, 3, 5, 10, 15, 25, 50, 100), limits=c(0, 105))+
  scale_y_continuous(labels = comma)

```

## How target variable changes in time
The revenue itself can be viewed as a timeseries. There seems to be a pattern of peaks.

```{r date, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
p1 <- tr %>% 
  bind_cols(as_tibble(y)) %>% 
  group_by(date) %>% 
  summarize(visits = n()) %>% 
  ungroup() %>% 
  ggplot(aes(x = date, y = visits)) + 
  geom_line() +
  geom_smooth() + 
  labs(x = "") +
  theme_minimal()

p2 <- tr %>% 
  bind_cols(as_tibble(y)) %>% 
  group_by(date) %>% 
  summarize(revenue = mean(value)) %>% 
  ungroup()  %>% 
  ggplot(aes(x = date, y = revenue)) + 
  geom_line() +
  stat_smooth() +
  labs(x = "") +
  theme_minimal()

multiplot(p1, p2, cols = 1)     
```


There is an interesting separation in target variable by **isTrueDirect** feature:
```{r target_time_isTD, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  group_by(date, isTrueDirect) %>% 
  summarize(revenue = mean(value)) %>% 
  ungroup()  %>% 
  ggplot(aes(x = date, y = revenue, color = isTrueDirect)) + 
  stat_smooth(aes(color = isTrueDirect)) +
  labs(x = "") +
  theme_minimal()
```

## Revenue forecasting
Let's see if we can predict log-transformed mean daily revenue using timeseries.
Here we use [zoo](https://cran.r-project.org/web/packages/zoo/index.html) and 
[forecast](https://cran.r-project.org/web/packages/forecast/index.html) packages
for timeseries modelling:
```{r fc1, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
tr %>% 
  bind_cols(tibble(revenue = y)) %>% 
  group_by(date) %>% 
  summarize(mean_revenue = log1p(mean(revenue/1e6))) %>% 
  ungroup() %>% 
  with(zoo(mean_revenue, order.by = date)) ->
  revenue
  
h <- max(te$date) - min(te$date) + 1
  
revenue %>% 
  autoplot() + 
  geom_line() +
  geom_smooth() + 
  labs(x = "", y = "log(revenue)") +
  theme_minimal()
```

We use simple auto.arima model. We need to forecast for the period of
`r h` days.

```{r fc2, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
m_aa <- auto.arima(revenue)
summary(m_aa)

forecast(m_aa, h = h) %>% 
  autoplot() + 
  theme_minimal()
```

Clearly, this model is of no use for long time period forecasting. 

Let's add a regression term **mean pageviews**:

```{r fc3, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
tr %>% 
  group_by(date) %>% 
  summarize(mean_pv = log1p(mean(pageviews, na.rm=TRUE))) %>% 
  ungroup() %$% 
  mean_pv ->
  mean_pv_tr

te %>% 
  group_by(date) %>% 
  summarize(mean_pv = log1p(mean(pageviews, na.rm=TRUE))) %>% 
  ungroup() %$% 
  mean_pv ->
  mean_pv_te
```

```{r fc4, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
m_aa_reg <- auto.arima(revenue, xreg = mean_pv_tr)
summary(m_aa_reg)

forecast(m_aa_reg, h = h, xreg = mean_pv_te) %>% 
  autoplot() + 
  theme_minimal()
```


## Time features
The dataset contains the timestamp column **visitStartTime** expressed as POSIX time.
It allows us to create a bunch of features. Let's check symmetric differences of the
time features from the train and test sets.

```{r tfea1, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
tr_vst <- as_datetime(tr$visitStartTime)
te_vst <- as_datetime(te$visitStartTime)

symdiff <- function(x, y) setdiff(union(x, y), intersect(x, y))
```
Year:
```{r tfea2, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
symdiff(tr_vst %>% year %>% unique, te_vst %>% year %>% unique)
```
Month:
```{r tfea3, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
symdiff(tr_vst %>% month %>% unique, te_vst %>% month %>% unique)
```
Day:
```{r tfea4, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
symdiff(tr_vst %>% day %>% unique, te_vst %>% day %>% unique)
```
Week:
```{r tfea5, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
symdiff(tr_vst %>% week %>% unique, te_vst %>% week %>% unique)
```
Day of the year:
```{r tfea6, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
symdiff(tr_vst %>% yday %>% unique, te_vst %>% yday %>% unique)
```
Hour:
```{r tfea7, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
symdiff(tr_vst %>% hour %>% unique, te_vst %>% hour %>% unique)
```
We can see that some time features (week, month, day of the year) from the train and test sets 
differ notably. Thus, they can cause overfitiing, but **year** and **hour** can be useful.

## Distribution of visits and revenue by attributes

```{r freq1, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  group_by(channelGrouping) %>% 
  summarize(visits = n(), mean_revenue = mean(value), total_revenue = sum(value)) %>% 
  ungroup() %>% 
  mutate(channelGrouping = reorder(channelGrouping, -visits)) %>% 
  data.table::melt(id.vars = c("channelGrouping")) %>% 
  ggplot(aes(channelGrouping, value, fill = variable)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::comma) +
  facet_wrap(~ variable, scales = "free") + 
  theme_minimal() +
  labs(x = "channel grouping", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position="none") 
```

```{r freq2, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  mutate(browser = factor(browser) %>% fct_lump(prop=0.01)) %>% 
  group_by(browser) %>% 
  summarize(visits = n(), mean_revenue = mean(value), total_revenue = sum(value)) %>% 
  ungroup() %>% 
  mutate(browser = reorder(browser, -visits)) %>% 
  data.table::melt(id.vars = c("browser")) %>% 
  ggplot(aes(browser, value, fill = variable)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::comma) +
  facet_wrap(~ variable, scales = "free") + 
  theme_minimal() +
  labs(x = "browser", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position="none") 
```

```{r freq3, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  mutate(operatingSystem = factor(operatingSystem) %>% fct_lump(prop=0.01)) %>% 
  group_by(operatingSystem) %>% 
  summarize(visits = n(), mean_revenue = mean(value), total_revenue = sum(value)) %>% 
  ungroup() %>% 
  mutate(operatingSystem = reorder(operatingSystem, -visits)) %>% 
  data.table::melt(id.vars = c("operatingSystem")) %>% 
  ggplot(aes(operatingSystem, value, fill = variable)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::comma) +
  facet_wrap(~ variable, scales = "free") + 
  theme_minimal() +
  labs(x = "operating system", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position="none") 
```

```{r freq4, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  mutate(deviceCategory = factor(deviceCategory) %>% fct_lump(prop=0.01)) %>% 
  group_by(deviceCategory) %>% 
  summarize(visits = n(), mean_revenue = mean(value), total_revenue = sum(value)) %>% 
  ungroup() %>% 
  mutate(deviceCategory = reorder(deviceCategory, -visits)) %>% 
  data.table::melt(id.vars = c("deviceCategory")) %>% 
  ggplot(aes(deviceCategory, value, fill = variable)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::comma) +
  facet_wrap(~ variable, scales = "free") + 
  theme_minimal() +
  labs(x = "device category", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position="none") 
```


# World map: important values

```{r map, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
highchart(type = "map") %>%
  hc_add_series_map(worldgeojson,
                    tr %>% 
                      bind_cols(as_tibble(y)) %>% 
                      group_by(country) %>% 
                      summarise(revenue = log1p(sum(value))) %>% 
                      ungroup() %>% 
                      mutate(iso2 = countrycode(country, origin="country.name", destination="iso2c")),
                    value = "revenue", joinBy = "iso2") %>%
  hc_title(text = "log Revenue by country") %>%
  hc_tooltip(useHTML = TRUE, headerFormat = "",
             pointFormat = "{point.country}: {point.revenue:.0f}") %>% 
  hc_colorAxis(minColor = "#e8eded", maxColor = "#4c735e")
             
highchart(type = "map") %>%
  hc_add_series_map(worldgeojson, 
                    tr %>% 
                      group_by(country) %>% 
                      summarise(pageviews = sum(pageviews)) %>% 
                      ungroup() %>% 
                      mutate(iso2 = countrycode(country, origin="country.name", destination="iso2c")),
                    value = "pageviews", 
                    joinBy = "iso2") %>%
  hc_title(text = "Pageviews by country") %>%
  hc_tooltip(useHTML = TRUE, headerFormat = "",
             pointFormat = "{point.country}: {point.pageviews}") %>% 
  hc_colorAxis(minColor = "#e8eded", maxColor = "#4c735e")

highchart(type = "map") %>%
  hc_add_series_map(worldgeojson, 
                    tr %>% 
                      group_by(country) %>% 
                      summarise(hits = sum(hits)) %>% 
                      ungroup() %>% 
                      mutate(iso2 = countrycode(country, origin="country.name", destination="iso2c")),
                    value = "hits", 
                    joinBy = "iso2") %>%
  hc_title(text = "Hits by country") %>%
  hc_tooltip(useHTML = TRUE, headerFormat = "",
             pointFormat = "{point.country}: {point.hits}") %>% 
  hc_colorAxis(minColor = "#e8eded", maxColor = "#4c735e")
```    

