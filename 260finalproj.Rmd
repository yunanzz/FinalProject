---
title: "BST260 Final Project RMarkdown"
author: "Yanran Li, Sean Gao, Yunan Zhao"
date: "2020/12/8"
output: html_document
---

### Customer Revenue Data Exploration
```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(ggplot2)
library(zoo)
library(h2o)
library(caret)
library(lme4)
library(ggalluvial)
library(xgboost)
library(jsonlite)
library(knitr)
library(Rmisc)
library(scales)
library(countrycode)
library(highcharter)
library(glmnet)
library(keras)
library(forecast)
library(magrittr)
library(mice) # for missing value
library(broom) # for tidy statistical objects
library(splines2) # for model smoothing
library(gam) # for model smoothing
```

### Load data
```{r, message=FALSE}
# Read the Customer Revenue data for further 

# for further reference:
# training data: train
# testing data: test
# submission data: subm

train <- read_csv("train.csv")
test <- read_csv("test.csv")
submission <- read_csv("sample_submission.csv")
```

### Reshaping Data
```{r}
set.seed(0)
head(test)
unique(train$socialEngagementType)
```

### Reshaping Data
```{r}
# printing out the first five rows of te and tr for further exploration
head(train,5)
head(test,5)

```

As we can see from above, device, geoNetwork, trafficSource, totals are all in JSON format, hence the next step we need to is to deal with the JSON data.

```{r message=FALSE, warning=FALSE, results='hide'}

# here since I am referring back to the link:
# https://www.kaggle.com/mrlong/r-flatten-json-columns-to-make-single-data-frame
# https://www.kaggle.com/kailex/r-eda-for-gstore-glm-keras-xgb
# in terms of transfroming the netted json file into single data frame
# as we can see from the above dataframe, for the json data, it is delimited by ,

train_device <- paste("[", paste(train$device, collapse = ","), "]") %>% fromJSON(flatten = T)
train_geoNetwork <- paste("[", paste(train$geoNetwork, collapse = ","), "]") %>% fromJSON(flatten = T)
train_totals <- paste("[", paste(train$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
train_trafficSource <- paste("[", paste(train$trafficSource, collapse = ","), "]") %>% fromJSON(flatten = T)

test_device <- paste("[", paste(test$device, collapse = ","), "]") %>% fromJSON(flatten = T)
test_geoNetwork <- paste("[", paste(test$geoNetwork, collapse = ","), "]") %>% fromJSON(flatten = T)
test_totals <- paste("[", paste(test$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
test_trafficSource <- paste("[", paste(test$trafficSource, collapse = ","), "]") %>% fromJSON(flatten = T)
```

```{r message=FALSE, warning=FALSE, results='hide'}
# sanity check upon the dealing with the json file
head(train_device, 5)
head(test_device, 5)
```

```{r message=FALSE, warning=FALSE, results='hide'}

# after we got the json-dealt file, we need to combine all of the columns for the four original columns
# here we will apply cbind, i.e column bind
# some codes idea come from this link:
# https://medium.com/coinmonks/merging-multiple-dataframes-in-r-72629c4632a3

train <- train  %>%  cbind(train_device, train_geoNetwork, train_totals, train_trafficSource )
# also we need to get rid of the original four columns
train <- train  %>%  select(-device,-geoNetwork, -trafficSource, -totals)


test <- test  %>%  cbind(test_device, test_geoNetwork, test_totals, test_trafficSource )
# also we need to get rid of the original four columns
test <- test  %>%  select(-device,-geoNetwork, -trafficSource, -totals) 

```


```{r message=FALSE, warning=FALSE, results='hide'}
# sanity check to see whether we get rid of all of the previous json problems
# also here we print out the number of columns for train and test
head(train, 5)
head(test, 5)
ncol(train)
ncol(test)
```


As we can see from the above checking, we do see that there are some null values, and the testing dataset has two fewer columns than the train ones
Therefore, we would like to further discover which are those different columns

```{r message=FALSE, warning=FALSE, results='hide'}
# sanity check to see whether we get rid of all of the previous json problems
# also here we print out the number of columns for train and test
train_col_names <- names(train)
test_col_names <- names(test)
# https://stackoverflow.com/questions/31573087/difference-between-two-vectors-in-r
setdiff(train_col_names, test_col_names)
```

As we can see from above, transactionRevenue is the target variable, which is for sure missing in the testing 
for campaignCode, for now I think it is okay for us to remove the corresponding column

```{r}

train <- train %>% select(-campaignCode)
```


```{r}
# double check to make sure the corresponding column has been deleted successfully
ncol(train)
```

### Now, let's deal with columns with constant values and missing values
```{r}
# reference: https://dplyr.tidyverse.org/reference/n_distinct.html
# here we want to return the number of unique values for each column in the dataset
col_unique_val <- sapply(train, n_distinct)
col_unique_val

# now, select all fo the values with only one distinct value, i.e those one with constant values
# for these columns, we will get rid of those
constant_col <- names(col_unique_val[col_unique_val==1])
# now we need to delete corresponding columns from both train and test
train <- train %>% select(-constant_col)
test <- test %>% select(-constant_col)
```


### Missing values

Before examine data analysis, we would like to deal with the missing values in our dataset. 
```{r}
sum(is.na(train))
sum(is.na(test))
```

Though the number of missing values in our dataset looks scary, it is probably owing to our large dataset. Besides, there are some colums that we do not really interested in, so we would like to locate how these missing values distribute in our dataset.

```{r nas0, result='asis', echo=TRUE}
# miss_train <- md.pattern(train, plot = FALSE, rotate.name = TRUE)
# m_train <- missing[order(missing[,36], decreasing = TRUE),]
# colnames(m_train)[36] <- "sum"
# m_train[1,]

```

Then, we do the same thing for the test dataset
```{r}
# miss_test <- md.pattern(test, plot = FALSE, rotate.name = TRUE)
#m_test <- missing[order(missing[,36],decreasing=TRUE),]
#colnames(m_test)[36] <- "sum"
#m_test[1,]

## Here, we print the number of missing values n each column, and it would be a reference about which variable that we would like to choose afterwards
```

## Data Wragling

We need to convert some features to their natural representation, including date, hits(integer, provides a record of all page visits), pageviews, bounces, newVisits, transactionRevenue.

```{r tf1, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
train <- train %>%
  mutate(date = ymd(date),
         hits = as.integer(hits),
         pageviews = as.integer(pageviews),
         bounces = as.integer(bounces),
         newVisits = as.integer(newVisits),
         transactionRevenue = as.numeric(transactionRevenue))
         
test <- test %>%
  mutate(date = ymd(date),
         hits = as.integer(hits),
         pageviews = as.integer(pageviews),
         bounces = as.integer(bounces),
         newVisits = as.integer(newVisits))

``` 


## Data Visualization

This dataset keeps track of the transaction revenue reported for cities and regions within a given country. We would like to only look at the overall and other sub-regions within a given country, so we create a new data frame with three columns: country name, date, and the transaction revenue in the country on that day. 

```{r}
library(plyr)
new_train <- train %>%
  select(c(country, date, transactionRevenue)) %>%
  group_by(country, date) %>%
  filter(!is.na(transactionRevenue ))
#  %>% summarize(cases = sum(transactionRevenue))
## total revenue 这里有点问题还
# 
# countries <- "United States"
# # new_train %>% filter(country == "United States") %>%
# #    ggplot(aes(x = date, y = transactionRevenue, color = country)) + 
# #    geom_line(na.rm = TRUE) +
# #    xlab("Date") + 
# #    ylab("Rransaction Revenue") +
# #    ggtitle("Global Data of the Transaction Revenue")
# 
new_train_US <- filter(new_train, country == "United States")

new_train_US %>%
   ggplot(aes(x = date, y = transactionRevenue, color = country)) +
   geom_line(na.rm = TRUE) +
   xlab("Date") +
   #ylab("Rransaction Revenue") +
   scale_y_log10("Rransaction Revenue (log10 scaled)")
   ggtitle("Global Data of the Transaction Revenue")
```

Using your new data frame, make a line plot of the Transactiion revenues vs. date for the following countries: the United States.

### Target variable
Firstly, we see **transactionRevenue** which is a sub-column of the **totals** JSON column. 
```{r target, result='asis', echo=TRUE}
y <- train$transactionRevenue
# train$transactionRevenue <- NULL
summary(y)
```

There exists 892138 NA values in **transactionRevenue**, which means only 1.27% rows have this value.


Secondly, we see **channelGrouping**, which is the channel via which the user came to the Store. We drew a histogram about the counts of channels among all the records in the training set.

```{r}
unique(train$channelGrouping)

train %>% ggplot(aes(channelGrouping)) + geom_bar() + ggtitle("The channel via which the user came to the Store") +
xlab("Channel") +
ylab("Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position="none") 
```

From the histogram above, the "Organic Search" had the most records, and then "Social", "Direct" followed.

```{r}


```


## Regression
After having some general ideas about our dataset, we would like to make statistical model about customers' revune. Since our dataset is too large, we would like to have a truncated dataset to by filtering the original dataset to avoid long runtimes and computer crashing.

Here, we are interested in the condition in the United States.

```{r}
revenue_us <- train %>%
  select(c(channelGrouping, date,
           fullVisitorId, sessionId, visitId,
           visitNumber, visitStartTime,
           isMobile, deviceCategory, hits, browser, source, 
           continent, subContinent, country, region, metro, city,
           transactionRevenue)) %>%
  filter(country == "United States")

write.csv(x = revenue_us, file = "revenue_us.csv")
```

The ourcome Y in our study is the continuous customer revenue, and we would like to do prediction about it based on cutomers' characteristics.

```{r}
str(revenue_us)
```

The column "hits" is a continuous variable provides record of all page vistis for the cusomer, and "isMobile" is a categorical variable indicates whether the customer used mobile device to access the Store. First, we want to visualze the relationship between these two explanatory variables and the outcome revenue.

```{r}
scatter.smooth(revenue_us$hits, revenue_us$transactionRevenue, col="pink",
               main = "Relation between hits and revenue",
               xlab = "Number of the page vist", ylab = "Transaction revenue")
plot(factor(revenue_us$isMobile), revenue_us$transactionRevenue, col="purple", lwd = 2)
pairs(~ hits + transactionRevenue + factor(isMobile), data = revenue_us, main = " ")
```

```{r}
# Here, we fit a simple linear model Revenue ~ Hits in accordance with the scatter plot above
lm_hits <- lm(revenue_us$transactionRevenue ~ revenue_us$hits)
summary(lm_hits)$coef
```
The coefficient of hits is statistically significant based on this regression model, which indicates that hits is associated with the output revenue.

```{r}
# Redefine isMobile variable for ggplot input and run this multiple linear model
revenue_us$isMobile[revenue_us$isMobile == "TRUE"] = 1
revenue_us$isMobile[revenue_us$isMobile == "FALSE"] = 0
lm2 <- lm(transactionRevenue ~ hits + isMobile, data = revenue_us)
summary(lm2)$coef
```

```{r, warning=FALSE}
# Here, we have the plot of the revenue ~ hits + device
p <- ggplot(data = revenue_us,
            aes(x = hits, y = transactionRevenue, color = factor(isMobile))) +
  geom_point() +
  ggtitle("Transaction Revenue by hits + device") +
  scale_color_manual(values = c("blue", 'hot pink')) +
  theme(legend.title = element_blank()) +
  scale_fill_manual(values = c("0", "1"), name="Access Device", breaks = c("0", "1"),
                    labels=c("Not Mobile", "Mobile"))
p
```
```{r}
# To further explore the relationship between revenue and hits, we start to consider degree polynomials
hits.2 <- I(revenue_us$hits^2)
pairs(~ hits + transactionRevenue + hits.2, data = revenue_us, main = "Pairs")
```
```{r}
# Fit multiple linear regression model include quadratic hits term
lm3_sq = lm(transactionRevenue ~ hits + I(hits^2) + isMobile, data = revenue_us)
tidy(lm3_sq)

# Fit multiple linear regression model with interaction term
lm4 = lm(transactionRevenue ~ hits + I(hits^2) + isMobile + hits*isMobile, data = revenue_us)
tidy(lm4)
```

Thought the p-values from previous models are all significant, we should notice that these linear regression model has several assumptions. Now, we would like to check whethere the main assumptions are violated.

```{r}
# Residuals versus fitted plot
plot(fitted(lm4), residuals(lm4))
abline(a=0, b=0, col="red", lwd=2)

# QQ Norm plot
qqnorm(residuals(lm4))
qqline(residuals(lm4), col="red", lwd=2)
```

From the scatterplot of Residuals versus fitted values, there are few discernable outliers. The Q-Q plot also departures from straight line, which is statistical evidence of non-normality. Since the assumptions do not hold, we need to check our data and develop more appropriate models for the given data.

```{r}
# To reduce the skewness in the residuals, we transform the output Y
lm5_log <- lm(log(transactionRevenue) ~ hits + I(hits^2) + isMobile, data = revenue_us)
tidy(lm5_log)
plot(fitted(lm5_log), residuals(lm5_log))
abline(a=0, b=0, col="3", lwd=2)
qqnorm(residuals(lm5_log))
qqline(residuals(lm5_log), col="3", lwd=2)
```

By apply log transformation for the transaction revenue, we achieve the linearity of effect, and normalize the residuals. Now, the regression model satisfies the assumptions better. 

```{r, warning=FALSE}
# After developing the relationship between revenue, hits, and device, we would like to consider including other forms of covariates into our regression model. Before doing that, we delve into the dataset again to see other potential estimators
scatter.smooth(revenue_us$visitStartTime, log(revenue_us$transactionRevenue),
               col = "brown",
               xlab = "Number of the page vist",
               ylab = "Transaction revenue",
               main = "Relation between visit starttime and revenue")

p2 <- ggplot(data = revenue_us,
            aes(x = channelGrouping, y = log(transactionRevenue))) +
              geom_boxplot() +
  xlab("Access channel") +
  ylab("Transaction revenue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
p2
```
According to the scatterplot and the boxplot, it seems that both visitStartTime and channelGroupng do not have significant influence on the value of transaction revenue, so we will keep using "hits" and "isMobile" as the predictors.

```{r}
# fit degree = df = 3, 6, 16
lm5.poly3 <- lm(log(transactionRevenue) ~ poly(hits, 3) + isMobile, data = revenue_us)
lm5.bs3 <- lm(log(transactionRevenue) ~ bs(hits, df = 3) + isMobile, data = revenue_us)
lm5.bs6 <- lm(log(transactionRevenue) ~ bs(hits, df = 6) + isMobile, data = revenue_us)
lm5.bs16 <- lm(log(transactionRevenue) ~ bs(hits, df = 16) + isMobile, data = revenue_us)

tidy(lm5.poly3)
tidy(lm5.bs3)
tidy(lm5.bs6)
tidy(lm5.bs16)
```

Beforehand, we did log transformation for the output Y, and here we would like to transform our predictors. By including predictive variable "hits" in the form of polynomial function with degree = 3, the adjusted R-squared increase indicating a better prediction to some extent. In addition, we apply Spline model to smooth the overall function, and the adjusted R-squared increases as we have higher degrees of freedom in this study.



